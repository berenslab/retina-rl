name: activation-functions
description: Testing how the presence of distractors effects network performance.
program: train.py
method: grid
project: retinal-rl
parameters:
  network:
    value: None
  env:
    value: "gathering_cifar"
  repeat:
    distribution: "int_uniform"
    min: 1
    max: 3
  num_workers:
    value: 24
  num_envs_per_worker:
    value: 8
  wide_aspect_ratio:
    value: False
  res_h:
    value: 90
  res_w:
    value: 120
  train_for_env_steps:
    value: 1000000000
  use_rnn:
    value: "False"
  rnn_size:
    value: 64
  with_vtrace:
    value: True
  exploration_loss_coeff:
    value: 0.001
  learning_rate:
    value: 0.0001
  lr_schedule:
    value: "constant"
  activation:
    values: ["elu", "relu", "tanh"]
  normalize_returns:
    value: False
  rollout:
    value: 32
  recurrence:
    value: 32
  experiment:
    value: "{activation}_{repeat}"
  train_dir:
    value: "./train_dir/activation-functions"
